{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel processing for Numpy Arrays and Pandas DataFrames\n",
    "\n",
    "http://dask.pydata.org/en/latest/\n",
    "\n",
    "Interesting example and use-case:\n",
    "\n",
    "http://matthewrocklin.com/blog/work/2018/02/09/credit-models-with-dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.distributed\n",
    "import dask.dataframe\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:39022\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>10.28 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:39022' processes=4 cores=4>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = dask.distributed.Client() #starts Dask client\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle Dask runs also without it, but it seems to be less efficient (?) and more difficult to monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrame\n",
    "\n",
    "__ToDo: refactor, use actual example data__\n",
    "\n",
    "The following example imports a large amount of data (from multiple zipped csv files) into a Dask dataframe, does some analysis and aggregation and returns a Pandas dataframe.\n",
    "\n",
    "Example data:\n",
    "\n",
    "http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dask.dataframe.read_csv(file_pattern,delimiter=',',decimal='.') #dask csv reader, \n",
    "# works on wildcards (e.g. *), but not on zip files\n",
    "agg=df.groupby(['date']).agg({'passenger_count':'sum', 'trip_distance':'sum',\n",
    "                              'tip_fraction':'mean'}) #groupby analogue to Pandas synthax\n",
    "agg_df=agg.compute() #creates Pandas DataFrame from Dask DataFrame and does the actual computing\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution progress and parallelism efficiency can be monitored in the web-gui of the Dask client:\n",
    "\n",
    "http://localhost:8787/status\n",
    "\n",
    "If something went wrong: kill all python tasks (Windows):\n",
    "\n",
    "tasklist\n",
    "taskkill /IM pythonw.exe /F\n",
    "\n",
    "In contrast to Pandas, Dask DataFrames cannot be directly created from zipped csv files. The following code snippet uses Pandas to import zipped files on Dask grid via Dask Delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = u'C:/Users/d90394/Documents/test_data/*.csv'\n",
    "files=glob(path)\n",
    "\n",
    "dfs=[dask.delayed(pandas.read_csv)(filename,delimiter=',',decimal='.')\n",
    "     for filename in files]\n",
    "df=dask.dataframe.from_delayed(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the import of a single zipped csv file is not distributed, thus the memory of each worker must be sufficient to contain the Pandas DataFrame for each input file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Delayed\n",
    "\n",
    "__ToDo: working, but not nice enough - needs refactoring__\n",
    "\n",
    "Dask Delayed is used to submit functions with defined input and output to the calculators. An example is given here:\n",
    "\n",
    "dask_delayed_example.py\n",
    "\n",
    "Note that the compute() statement is only executed for the result(s), the dependencies are handled automatically by Dask.\n",
    "\n",
    "If not a single function but a class is intended to be submitted, a helper function can be used. If the helper function is defined inside a class, it should be static (i.e. without referring to self)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A: #class which does the calculation, to be parallelized\n",
    "    def __init__(self, p1,x1):\n",
    "        self.p1=p1\n",
    "        self.x1=x1\n",
    "    def doCalc(self,p2,x2):\n",
    "        return np.dot(self.p1+self.x1,p2+x2)\n",
    "\n",
    "# ToDo: working, but not nice enough - needs refactoring\n",
    "class B: #class which calls the calculation\n",
    "    def mainProg(self):\n",
    "        p1=np.array(range(10000000))\n",
    "        p2=p1**2\n",
    "        p1s=client.scatter(p1) #distribute large data, which is required for all calculations, ahead to the calculators\n",
    "        p2s=client.scatter(p2)\n",
    "        self.result=[]\n",
    "        for x1 in range(100):\n",
    "            x2=x1**2\n",
    "            self.result.append(dask.delayed(self.calcHelper)(p1s,p2s,x1,x2))\n",
    "        result_tot=dask.delayed(np.sum)(self.result)\n",
    "        return result_tot.compute()\n",
    "\n",
    "    @staticmethod\n",
    "    def calcHelper(p1,p2,x1,x2): #helper function defined as staticmethod\n",
    "        a=A(p1,x1)\n",
    "        return a.doCalc(p2,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2201258113697798912\n",
      "6.076910000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# ToDo: refactor\n",
    "start_time=clock()\n",
    "b=B()\n",
    "print(b.mainProg())\n",
    "elapsed_time=clock()-start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the command xs=client.scatter(x) is used to distribute large data effectively to the calculators.\n",
    "\n",
    "If there are issues with multiprocessing, the client can be restricted to multithreading (which is usually slower) using\n",
    "\n",
    "client=dask.distributed.Client(processes=False, threads_per_worker=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
